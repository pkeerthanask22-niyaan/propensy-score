{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nF4yA-Ek-W4j"
      },
      "outputs": [],
      "source": [
        "# %%\n",
        "# Imports and configuration\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Configurable parameters\n",
        "N = 5000\n",
        "N_CONFOUNDERS = 6\n",
        "TREATMENT_EFFECT = 2.0\n",
        "HETEROGENEOUS = True          # if True, tau varies with X; else constant\n",
        "CALIPER_SD_UNITS = 0.2        # caliper in SD units of logit(p_hat)\n",
        "MATCH_REPLACE = False         # matching without replacement\n",
        "POLY_DEGREE_OUTCOME = 2      # degree for polynomial features in outcome model for AIPW\n",
        "BOOTSTRAP_B = 200             # bootstrap iterations for AIPW se\n",
        "OUT_DIR = \"/mnt/data/causal_project_outputs\"  # change if needed\n",
        "RNG_SEED = 42\n",
        "\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "# %%\n",
        "# 1) Synthetic data generation\n",
        "def generate_synthetic_data(n=N, n_conf=N_CONFOUNDERS, treatment_effect=TREATMENT_EFFECT, hetero=HETEROGENEOUS, seed=RNG_SEED):\n",
        "    np.random.seed(seed)\n",
        "    X = np.random.normal(0, 1, size=(n, n_conf))\n",
        "    # introduce correlation and nonlinear transforms\n",
        "    X[:, 1] = 0.6 * X[:, 0] + 0.8 * np.random.normal(size=n)\n",
        "    nonlin = np.column_stack([\n",
        "        np.sin(X[:, 0]),\n",
        "        X[:, 1]**2,\n",
        "        np.exp(0.2 * X[:, 2]),\n",
        "        np.tanh(X[:, 3]),\n",
        "        X[:, 4] * X[:, 5 % n_conf]\n",
        "    ])\n",
        "    logits = (-0.2 + 0.8 * X[:, 0]\n",
        "              - 0.6 * (X[:, 1]**2)\n",
        "              + 0.5 * np.sin(X[:, 2])\n",
        "              + 0.3 * X[:, 3]\n",
        "              + 0.4 * nonlin[:, 1]\n",
        "              - 0.3 * nonlin[:, 3])\n",
        "    p = 1.0 / (1.0 + np.exp(-logits))\n",
        "    T = np.random.binomial(1, p, size=n)\n",
        "    mu0 = (1.5 * X[:, 0] - 0.7 * (X[:, 1]**2)\n",
        "           + 0.9 * np.sin(X[:, 2]) + 0.5 * X[:, 3] + 0.3 * X[:, 4] + 0.4 * nonlin[:, 2])\n",
        "    if hetero:\n",
        "        tau = treatment_effect + 0.5 * np.tanh(X[:, 0]) - 0.3 * X[:, 1]\n",
        "    else:\n",
        "        tau = np.full(n, treatment_effect)\n",
        "    mu1 = mu0 + tau\n",
        "    Y = mu0 + T * tau + np.random.normal(0, 1.0, size=n)\n",
        "    df = pd.DataFrame(X, columns=[f\"X{i+1}\" for i in range(n_conf)])\n",
        "    df['T'] = T\n",
        "    df['Y'] = Y\n",
        "    df['true_propensity'] = p\n",
        "    df['true_mu0'] = mu0\n",
        "    df['true_mu1'] = mu1\n",
        "    return df\n",
        "\n",
        "# Generate dataset\n",
        "df = generate_synthetic_data()\n",
        "feature_cols = [c for c in df.columns if c.startswith(\"X\")]\n",
        "\n",
        "# %%\n",
        "# 2) Fit propensity model (logistic regression)\n",
        "def fit_propensity_model(df, feature_cols):\n",
        "    X = df[feature_cols].values\n",
        "    y = df['T'].values\n",
        "    model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
        "    model.fit(X, y)\n",
        "    p_hat = model.predict_proba(X)[:, 1]\n",
        "    eps = 1e-8\n",
        "    logit = np.log((p_hat + eps) / (1 - p_hat + eps))\n",
        "    df = df.copy()\n",
        "    df['p_hat'] = p_hat\n",
        "    df['logit_p_hat'] = logit\n",
        "    return df, model\n",
        "\n",
        "df, prop_model = fit_propensity_model(df, feature_cols)\n",
        "\n",
        "# %%\n",
        "# 3) Nearest Neighbor Matching on logit(p_hat) with caliper\n",
        "def nearest_neighbor_match_on_logit(df, caliper_sd_units=CALIPER_SD_UNITS, replace=MATCH_REPLACE):\n",
        "    treated = df[df['T'] == 1].copy().reset_index()\n",
        "    control = df[df['T'] == 0].copy().reset_index()\n",
        "    sd_logit = df['logit_p_hat'].std()\n",
        "    cal = caliper_sd_units * sd_logit\n",
        "    nbrs = NearestNeighbors(n_neighbors=1).fit(control[['logit_p_hat']])\n",
        "    distances, indices = nbrs.kneighbors(treated[['logit_p_hat']])\n",
        "    distances = distances.ravel()\n",
        "    indices = indices.ravel()\n",
        "    matched_t_idx = []\n",
        "    matched_c_idx = []\n",
        "    used_c = set()\n",
        "    for t_pos, (d, c_pos) in enumerate(zip(distances, indices)):\n",
        "        if d <= cal:\n",
        "            orig_t_idx = treated.loc[t_pos, 'index']\n",
        "            orig_c_idx = control.loc[c_pos, 'index']\n",
        "            if (not replace) and (c_pos in used_c):\n",
        "                continue\n",
        "            matched_t_idx.append(orig_t_idx)\n",
        "            matched_c_idx.append(orig_c_idx)\n",
        "            used_c.add(c_pos)\n",
        "    pairs_df = pd.DataFrame({'treated_idx': matched_t_idx, 'control_idx': matched_c_idx})\n",
        "    matched_list = []\n",
        "    for gid, row in pairs_df.reset_index().iterrows():\n",
        "        trow = df.loc[row['treated_idx']].copy()\n",
        "        trow['match_group'] = gid\n",
        "        crow = df.loc[row['control_idx']].copy()\n",
        "        crow['match_group'] = gid\n",
        "        matched_list.append(trow)\n",
        "        matched_list.append(crow)\n",
        "    if matched_list:\n",
        "        matched_df = pd.DataFrame(matched_list).reset_index(drop=True)\n",
        "    else:\n",
        "        matched_df = pd.DataFrame(columns=df.columns.tolist() + ['match_group'])\n",
        "    return matched_df, pairs_df, cal\n",
        "\n",
        "matched_df, pairs_df, cal_used = nearest_neighbor_match_on_logit(df)\n",
        "\n",
        "# %%\n",
        "# 4) Standardized Mean Differences (SMD)\n",
        "def standardized_mean_difference(df_t, df_c, cols):\n",
        "    smd = {}\n",
        "    for col in cols:\n",
        "        m_t = df_t[col].mean()\n",
        "        m_c = df_c[col].mean()\n",
        "        s_t = df_t[col].var(ddof=1)\n",
        "        s_c = df_c[col].var(ddof=1)\n",
        "        pooled = np.sqrt((s_t + s_c) / 2.0)\n",
        "        smd[col] = (m_t - m_c) / (pooled + 1e-10)\n",
        "    return pd.Series(smd)\n",
        "\n",
        "smd_before = standardized_mean_difference(df[df['T'] == 1], df[df['T'] == 0], feature_cols)\n",
        "if len(matched_df) > 0:\n",
        "    smd_after = standardized_mean_difference(matched_df[matched_df['T'] == 1], matched_df[matched_df['T'] == 0], feature_cols)\n",
        "else:\n",
        "    smd_after = pd.Series({col: np.nan for col in feature_cols})\n",
        "smd_df = pd.DataFrame({'SMD_before': smd_before, 'SMD_after': smd_after})\n",
        "\n",
        "# %%\n",
        "# 5) PSM ATE from matched pairs (diff-in-means)\n",
        "def psm_ate_from_pairs(df, pairs_df):\n",
        "    diffs = []\n",
        "    for _, row in pairs_df.iterrows():\n",
        "        y_t = df.loc[row['treated_idx'], 'Y']\n",
        "        y_c = df.loc[row['control_idx'], 'Y']\n",
        "        diffs.append(y_t - y_c)\n",
        "    diffs = np.array(diffs)\n",
        "    if len(diffs) == 0:\n",
        "        return np.nan, np.nan\n",
        "    return diffs.mean(), diffs.std(ddof=1) / np.sqrt(len(diffs))\n",
        "\n",
        "psm_ate, psm_se = psm_ate_from_pairs(df, pairs_df)\n",
        "\n",
        "# %%\n",
        "# 6) Matched-sample regression (Y ~ T + X)\n",
        "def matched_regression_ate(matched_df, feature_cols):\n",
        "    X_mat = matched_df[feature_cols + ['T']]\n",
        "    y_mat = matched_df['Y'].values\n",
        "    X_sm = sm.add_constant(X_mat)\n",
        "    res = sm.OLS(y_mat, X_sm).fit(cov_type='HC1')\n",
        "    coef_T = res.params['T']\n",
        "    se_T = res.bse['T']\n",
        "    return float(coef_T), float(se_T), res\n",
        "\n",
        "if len(matched_df) > 0:\n",
        "    matched_ate, matched_se, matched_res = matched_regression_ate(matched_df, feature_cols)\n",
        "else:\n",
        "    matched_ate, matched_se, matched_res = np.nan, np.nan, None\n",
        "\n",
        "# %%\n",
        "# 7) Doubly Robust AIPW estimator\n",
        "def aipw_estimator(df, feature_cols, poly_degree=POLY_DEGREE_OUTCOME, bootstrap_B=BOOTSTRAP_B, seed=RNG_SEED):\n",
        "    eps = 1e-8\n",
        "    poly = PolynomialFeatures(degree=poly_degree, include_bias=False)\n",
        "    X_poly = poly.fit_transform(df[feature_cols])\n",
        "    X_for_outcome = np.column_stack([df['T'].values.reshape(-1, 1), X_poly])\n",
        "    out_model = LinearRegression()\n",
        "    out_model.fit(X_for_outcome, df['Y'].values)\n",
        "    m1 = out_model.predict(np.column_stack([np.ones(len(df)).reshape(-1, 1), X_poly]))\n",
        "    m0 = out_model.predict(np.column_stack([np.zeros(len(df)).reshape(-1, 1), X_poly]))\n",
        "    p = df['p_hat'].values\n",
        "    T = df['T'].values\n",
        "    Y = df['Y'].values\n",
        "    aipw_scores = (T * (Y - m1) / (p + eps)) - ((1 - T) * (Y - m0) / (1 - p + eps)) + (m1 - m0)\n",
        "    ate = aipw_scores.mean()\n",
        "    rng = np.random.default_rng(seed)\n",
        "    boot_ates = []\n",
        "    n = len(df)\n",
        "    for b in range(bootstrap_B):\n",
        "        idx = rng.integers(0, n, n)\n",
        "        boot_ates.append(aipw_scores[idx].mean())\n",
        "    se = float(np.std(boot_ates, ddof=1))\n",
        "    return float(ate), se\n",
        "\n",
        "aipw_ate, aipw_se = aipw_estimator(df, feature_cols)\n",
        "\n",
        "# %%\n",
        "# 8) Naive and true ATE (for comparison)\n",
        "naive_ate = float(df[df['T'] == 1]['Y'].mean() - df[df['T'] == 0]['Y'].mean())\n",
        "true_ate = float((df['true_mu1'] - df['true_mu0']).mean())\n",
        "\n",
        "# %%\n",
        "# 9) Save dataset and concise report\n",
        "data_outfile = os.path.join(OUT_DIR, \"synthetic_data.csv\")\n",
        "report_outfile = os.path.join(OUT_DIR, \"this_report.txt\")\n",
        "df.to_csv(data_outfile, index=False)\n",
        "\n",
        "report_lines = []\n",
        "report_lines.append(\"Project: Propensity Score Matching & Doubly Robust Estimation\")\n",
        "report_lines.append(f\"Dataset: Synthetic (N={len(df)}), {len(feature_cols)} continuous confounders\")\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(\"1) Data generation\")\n",
        "report_lines.append(f\" - Heterogeneous treatment effect: {HETEROGENEOUS}\")\n",
        "report_lines.append(f\" - True ATE (population): {true_ate:.6f}\")\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(\"2) Propensity score modeling & matching\")\n",
        "report_lines.append(f\" - Logistic regression used for propensity estimation.\")\n",
        "report_lines.append(f\" - Caliper on logit(p_hat) used: {cal_used:.6f} (logit units); caliper parameter (SD units) = {CALIPER_SD_UNITS}\")\n",
        "report_lines.append(f\" - Matched pairs retained: {len(pairs_df)}\")\n",
        "report_lines.append(\" - SMDs before vs after matching:\")\n",
        "for col in feature_cols:\n",
        "    report_lines.append(f\"    - {col}: before={smd_df.loc[col,'SMD_before']:.3f}, after={smd_df.loc[col,'SMD_after']:.3f}\")\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(\"3) ATE estimators and results\")\n",
        "report_lines.append(f\" - Naive (unadjusted) diff-in-means: {naive_ate:.6f}\")\n",
        "report_lines.append(f\" - PSM (matched difference-in-means): {psm_ate:.6f} (SE {psm_se:.6f})\")\n",
        "report_lines.append(f\" - Matched-sample regression (Y ~ T + X): {matched_ate:.6f} (SE {matched_se:.6f})\")\n",
        "report_lines.append(f\" - Doubly Robust AIPW estimator: {aipw_ate:.6f} (bootstrap SE {aipw_se:.6f})\")\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(\"Interpretation:\")\n",
        "report_lines.append(\" - Matching reduces covariate imbalance (SMDs close to zero after matching).\")\n",
        "report_lines.append(\" - The naive estimator is biased; adjusted methods move toward the true ATE.\")\n",
        "report_lines.append(\"\")\n",
        "report_lines.append(f\"Files saved to: {OUT_DIR}\")\n",
        "report_lines.append(\" - synthetic_data.csv\")\n",
        "report_lines.append(\" - this_report.txt\")\n",
        "\n",
        "with open(report_outfile, \"w\") as f:\n",
        "    f.write(\"\\n\".join(report_lines))\n",
        "\n",
        "# %%\n",
        "# 10) Print concise summary\n",
        "print(\"Summary of results:\")\n",
        "print(f\" True ATE: {true_ate:.6f}\")\n",
        "print(f\" Naive ATE: {naive_ate:.6f}\")\n",
        "print(f\" PSM ATE: {psm_ate:.6f} (SE {psm_se:.6f})\")\n",
        "print(f\" Matched regression ATE: {matched_ate:.6f} (SE {matched_se:.6f})\")\n",
        "print(f\" AIPW ATE: {aipw_ate:.6f} (SE {aipw_se:.6f})\")\n",
        "print(\"\")\n",
        "print(\"SMDs before vs after matching:\")\n",
        "print(smd_df)\n",
        "print(\"\")\n",
        "print(\"Files saved:\", os.listdir(OUT_DIR))\n",
        "print(\"Dataset saved to:\", data_outfile)\n",
        "print(\"Report saved to:\", report_outfile)\n"
      ]
    }
  ]
}